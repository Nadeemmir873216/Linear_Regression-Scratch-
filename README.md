# ğŸ“Š Linear Regression from Scratch

This project contains a simple Python implementation of:

- **Simple Linear Regression**
- **Multiple Linear Regression**

Implemented entirely from scratch using only basic libraries like `NumPy`.

## ğŸš€ Whatâ€™s Inside

- ğŸ“ˆ **Simple Linear Regression** â€“ Predict using one feature (X â†’ Y)
- ğŸ“Š **Multiple Linear Regression** â€“ Predict using multiple features (Xâ‚, Xâ‚‚, ..., Xâ‚™ â†’ Y)
- ğŸ§® Manual gradient descent (no scikit-learn)
- ğŸ“‰ Loss function calculation and model evaluation

## ğŸ› ï¸ How to Use

1. **Clone the repository**
   ```bash
   git clone https://github.com/Nadeemmir873216/Linear_Regression-Scratch-.git
   cd Linear_Regression-Scratch-

2. Install dependencies

pip install numpy


3. Run the code

Open .ipynb notebook.

Edit the data as needed inside the code or load from a CSV.

Observe training outputs like weights, predictions, and error.




ğŸ“ File Structure
   ```bash
   Linear_Regression-Scratch-/
   â”œâ”€â”€ linearRegressionScikit.ipynb      # One feature implementation
   â”œâ”€â”€ MultipleLinearRegression.ipynb   # Multi-feature implementation
   â””â”€â”€ README.md
   ```

ğŸ’¡ Key Concepts Covered

Gradient Descent

Mean Squared Error (MSE)

Weight updates

Multiple variable regression


ğŸ“š Dependencies

numpy

âœ… Example

Given input features and target values, the model will learn weights to predict new outputs. All training steps are handled manually without ML libraries.


---

ğŸ‘¨â€ğŸ’» Author

Nadeem Mir


Feel free to fork, use, or contribute!

Let me know if you want to include plots, comparison with sklearn, or a live Colab notebook link!

