# 📊 Linear Regression from Scratch

This project contains a simple Python implementation of:

- **Simple Linear Regression**
- **Multiple Linear Regression**

Implemented entirely from scratch using only basic libraries like `NumPy`.

## 🚀 What’s Inside

- 📈 **Simple Linear Regression** – Predict using one feature (X → Y)
- 📊 **Multiple Linear Regression** – Predict using multiple features (X₁, X₂, ..., Xₙ → Y)
- 🧮 Manual gradient descent (no scikit-learn)
- 📉 Loss function calculation and model evaluation

## 🛠️ How to Use

1. **Clone the repository**
   ```bash
   git clone https://github.com/Nadeemmir873216/Linear_Regression-Scratch-.git
   cd Linear_Regression-Scratch-

2. Install dependencies

pip install numpy


3. Run the code

Open .ipynb notebook.

Edit the data as needed inside the code or load from a CSV.

Observe training outputs like weights, predictions, and error.




📁 File Structure
   ```bash
   Linear_Regression-Scratch-/
   ├── linearRegressionScikit.ipynb      # One feature implementation
   ├── MultipleLinearRegression.ipynb   # Multi-feature implementation
   └── README.md
   ```

💡 Key Concepts Covered

Gradient Descent

Mean Squared Error (MSE)

Weight updates

Multiple variable regression


📚 Dependencies

numpy

✅ Example

Given input features and target values, the model will learn weights to predict new outputs. All training steps are handled manually without ML libraries.


---

👨‍💻 Author

Nadeem Mir


Feel free to fork, use, or contribute!

Let me know if you want to include plots, comparison with sklearn, or a live Colab notebook link!

